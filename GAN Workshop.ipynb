{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introduction to Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "According to Yann LeCun, “adversarial training is the coolest thing since sliced bread”. I’m inclined to believe so because I don’t think sliced bread ever created this much buzz and excitement within the deep learning community.\n",
    "\n",
    "Generative Adversarial Networks are a set of models that basically learn to create synthetic data that is similar to input data it's given. In more formal terms, a GAN is a generative model that learns the probability distribution (or data distribution) of the training examples it is given. From this distribution, we can then create sample outputs. GANs have seen their largest progress with image training examples, but this idea of modeling data distributions is one that can be applied with other forms of input. In the case described in today’s post, we’ll be creating a GAN that learns to generate synthetic, yet readable, images of MNIST digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Basic GAN Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The basic structure of the model is simple. You have a Generator Network component and a Discriminator Network component, as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![\"Basic GAN Architecture\"](gan_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The job of the discriminator network, like the name suggests, is to discriminate between \"real\" images (like those from our dataset) from those that have been artificially created. It is implemented as a neural network, in our example below, that outputs one value representing the probability that the given image is real.\n",
    "\n",
    "The generator, on the other hand, will try to generate images to \"trick\" the disciminator, i.e. creating images that look natural or close to real. To achieve this goal, it will eventually learn to output images/data close to the original data distribution.  \n",
    "\n",
    "This can be thought of as a zero-sum or minimax two player game. The analogy used in the paper is that the generative model is like “a team of counterfeiters, trying to produce and use fake currency” while the discriminative model is like “the police, trying to detect the counterfeit currency”. The generator is trying to fool the discriminator while the discriminator is trying to not get fooled by the generator. As the models train through alternating optimization, both methods are improved until a point where the “counterfeits are indistinguishable from the genuine articles”. There are specific game theory concepts that prove there is indeed an equilibrium to this game where the generator gets so good that the discriminator outputs a probability of ½ for every input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Our Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Today, our goal is to use the MNIST data set as our \"real\" images to create a GAN model that can generate realistic images of hand drawn digits. The digits will have to look natural enough to fool a classifier that is trying to ditinguish between synthetic and natural images. Ideally, the produced images would be so realistic, that humans cannot distinguish them either.\n",
    "\n",
    "What we will need:\n",
    "\n",
    "* Real MNIST training images\n",
    "* A generator network that takes in a random noise vector and produces a synthetic image\n",
    "* A discriminator network that learns to distinguish between real and synthetic images, i.e. a binary classifier (1 for real image, 0 for fake)\n",
    "* An optimization procedure that jointly updates both networks through gradient descent.\n",
    "* Tensorflow - Our choice of Deep Learning framework\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, we must add out imports. We will mainly be working with Tensorflow, so we have to import that of course. The next line imports the MNIST data set we will be working with. Numpy is always useful when working with matrices, and in this case, to supply us with a random number generator function. Matplotlib gives us the ability to easily visualize our data as graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# canonical import statements\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Placeholders\n",
    "Next, we create variable placeholders for the inputs to our two models. The purpose of a placeholder is basically to tell Tensorflow \"We're going to input in our random z vector later, but for now, we're going to define this placeholder variable instead\". It lets Tensorflow know about the size of the inputs beforehand. For example, the shape of the placeholder for our generator will be [None x 100]. The None keyword means that the value can be determined at session runtime. We normally have None as our first dimension so that we can have variable batch sizes (With a batch size of 16, the input to the generator would be 16 x 100). With the None keywoard, we don't have to specify batch_size until later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# images (for the discriminator)\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "# noise vector (for the generator)\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Weight Initialization\n",
    "Before we create our model, it's a good idea to decide on how to initialize our weights. How weights are initialized can affect how effectively our model converges. Here, we have decided to initialize them with \"Xavier initialization.\"\n",
    "\n",
    "A good motivation for this is as follows: \"With each passing layer, we want the variance to remain the same. This helps us keep the signal from exploding to a high value or vanishing to zero. In other words, we need to initialize the weights in such a way that the variance remains the same for x and y. This initialization process is known as Xavier initialization.\" You can read more about the math and intuitiion [here](https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/):\n",
    "\n",
    "Below is a function we have defined for later use to help us initialize weights from a Gaussian distribution in \"xavier\" fashion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def xavier(shape):\n",
    "    return tf.truncated_normal(shape = shape, stddev = 1.0/tf.sqrt(shape[0]/2.0)) #\"xavier\" initialization of weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![\"Discriminator Network Visual Represenation\"](discriminator_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we shall create a simple neural network to act as our discriminator model. Our image we input is 28x28, so a total of 784 input data per image. Thus, our network will have an input layer of size 784, with a hidden layer of size 128, and ultimately output an layer of just 1 single scalar number activation to describe the probability that the image is real. \n",
    "\n",
    "We initialize our weights and our biases in \"__init__\" with xavier initialization for the former and setting all values to zero for the latter. Our \"discriminator\" function multiplies our weights and inputs accordingly across the layers to give an appropriate output. Finally, \"get_trainable_vars\" returns the weight and bias variables associated to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class discriminator_network:\n",
    "    \"\"\"MNIST IMAGE(s): x * 784 -> 128 hidden units -> 1 output neuron (probability of being real)\"\"\"\n",
    "    def __init__(self):\n",
    "        self.d_w1 = tf.Variable(xavier([784,128]))\n",
    "        self.d_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "        self.d_w2 = tf.Variable(xavier([128,1]))\n",
    "        self.d_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "    \n",
    "    def discriminator(self, x):\n",
    "        \"\"\"Calculate D(x)\"\"\"\n",
    "        d_hfc_1 = tf.nn.relu(tf.matmul(x, self.d_w1) + self.d_b1)\n",
    "        d_logit = tf.matmul(d_hfc_1, self.d_w2) + self.d_b2\n",
    "        d_prob = tf.nn.sigmoid(d_logit) # convert the output to a probability\n",
    "        return d_prob, d_logit\n",
    "    \n",
    "    def get_trainable_vars(self):\n",
    "        return [self.d_w1, self.d_b1, self.d_w2, self.d_b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The generator can be thought of as doing the \"opposite\" of the discriminator. It will be taking a random noise vector, and \"upsampling\" it to become a 28x28 \"image\" that the model learns to generate in a more realistic manner. \n",
    "\n",
    "The neural network will take an input size of 100 in its first layer, expand it into a hidden layer of size 128, and ultimately creating an output layer with the same size as the MNIST image data, 784.\n",
    "\n",
    "Once again, our \"__init__\" sets the weights and biases, the generator returns the appropriate outputs from the network, and \"get_trainable_vars\" returns the weight and bias variables of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class generator_network:\n",
    "    \"\"\"Random noise vector (100 dim assumed) -> expand to 128 units -> output 784 units (MNIST dim)\"\"\"\n",
    "    def __init__(self):\n",
    "        self.g_w1 = tf.Variable(xavier([100, 128])) # 100d noise vector assumed. Output 128 hidden units in first layer\n",
    "        self.g_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "        self.g_w2 = tf.Variable(xavier([128, 784])) # 784 outputs\n",
    "        self.g_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "    \n",
    "    def generator(self, z):\n",
    "        \"\"\"Calculate and sample G(z)\"\"\"\n",
    "        g_hfc_1 = tf.nn.relu(tf.matmul(z, self.g_w1) + self.g_b1) # 100 * 128 here\n",
    "        return tf.nn.sigmoid(tf.matmul(g_hfc_1, self.g_w2) + self.g_b2)\n",
    "    \n",
    "    def get_trainable_vars(self):\n",
    "        return [self.g_w1, self.g_b1, self.g_w2, self.g_b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "How do we get the random noise vector for the generator you might ask? We make a function for it below, using Numpy's numpy.random library. We want a uniform distribution from range -1.0 to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# next, we need a function to actually generate a 100d noise vector to feed into our generator\n",
    "def rand_noise_vector(num_vectors, size):\n",
    "    return np.random.uniform(-1.0, 1.0, size = [num_vectors, size]) # we might want a bunch of these to generate many imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Also, it would be really boring to generate all these images if we couldn't see any of it, so we write a function to plot our image data with matplotlib.pyplot. We can also save our figures with savefig( ) if we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# a function to plot the genned images\n",
    "def plot(samples, cur_epoch = None):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "        # if epoch is not specified we just overwrite the existing image\n",
    "        plt.savefig(\"gan{}\".format(\"\" if cur_epoch is None else cur_epoch))\n",
    "        plt.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here's the important part, where we define our networks and cost functions. First, let's create instances of both the generator and discriminator network, and sample a generated image, an output prediction on a real image, and an output prediction on a fake image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create networks\n",
    "gen_net, discriminator_net = generator_network(), discriminator_network()\n",
    "# compute G(z) where z is the random noise vector \n",
    "g_sample = gen_net.generator(z=Z) # this is G(z)\n",
    "# compute d(real) = p(image being real)\n",
    "_, d_logit_real = discriminator_net.discriminator(X) # this is D(x)\n",
    "# compute d(fake) = p(image being real)\n",
    "_, d_logit_fake = discriminator_net.discriminator(g_sample) # this is D(G(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As mentined in the [slides](https://docs.google.com/presentation/d/17sHIVbt8WN7fEUs9rnhwV6god4493xEjUt9aCYJ4BYM/edit?usp=sharing) we'd like the optimize the generator such that $D(G(z))$ is high. This is equivalent to maximizing the log probability $\\log(D(G(z))$, or minimizing $1 - \\log(D(G(z))$. The `tf.nn.sigmoid_cross_entropy` function will normalize our output logits from our generator and compute this loss function for us. Intuitively, we'd want the cross entropy between $D(G(z))$ and a vector of $1$'s to be zero, this would indicate that we've fooled the discriminator.\n",
    "\n",
    "The reduce mean function just takes the mean value of all of the components in the matrix returned by the cross entropy function. This is just a way of reducing the loss to a single scalar value, instead of a vector or matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train the generator w/fake logits\n",
    "# cross entropy between D(G(z))\n",
    "g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = d_logit_fake, labels = tf.ones_like(d_logit_fake)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let’s think about the discriminator’s point of view. We'd like to optimize is so that $D(x)$ is high and $D(G(z))$ is small, so we can try to maximize $\\log D(x) + \\log(1 - D(G(z))$. This means that we'd like the cross entropy between $D(x)$ and a vector of $1$ 1's to be zero, and the cross entropy between $D(G(z))$ and a vector of $0$'s to be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# optimize wrspt to the real logits, so all tha labels are one since we knew they came from real samples\n",
    "d_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logit_real, labels = tf.ones_like(d_logit_real)))\n",
    "# optimize wrspt to the fake logits, so all the labels are zero since we knew that they came from fake (generated) samples\n",
    "d_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = d_logit_fake, labels = tf.zeros_like(d_logit_fake)))\n",
    "# total loss is just the sum\n",
    "d_loss = d_real_loss + d_fake_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we specify our optimizers. We'll use the Adam Optimizer, which provides an adaptive learning rate and momentum out of the box. We'll also make sure that if we're optimizing the discriminator, then the generator variables are held constant, and vice versa. We'll do this by explicitly passing in a `var_list` to each `minimize` call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# make sure to only train w/relevant vars\n",
    "adam = tf.train.AdamOptimizer()\n",
    "d_step = adam.minimize(d_loss, var_list = discriminator_net.get_trainable_vars())\n",
    "g_step = adam.minimize(g_loss, var_list = gen_net.get_trainable_vars())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Wait... what are we training on again? Oh right! MNIST! Almost forgot...\n",
    "\n",
    "We should probably load our MNIST data set then. To do this, we’ll call a TF function called read_data_sets. This loads in the 55,000 training examples in the MNIST database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### MNIST, MNIST, MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ../../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ../../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ../../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ../../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Train and Run Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Okay, let's run finally run our model. We will create a Tensorflow session with tf.Session()\n",
    "\n",
    "We will train is with 100,000 epochs, and print out its progress every 1000 epochs, along with a sample image produced by our current generator network. \n",
    "\n",
    "In the end of our training, we print out 16 sample synthetic images. Wow... so natural, so real, so pleasing, so aesthetic. Don't they look like REAL hand written numbers? Real, human-like ugly handwriting, produced by your own machine learning model. Good job! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Discriminator loss: 1.522985577583313\n",
      "Generator loss: 1.7351818084716797\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAABJCAYAAABxcwvcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11527da58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000\n",
      "Discriminator loss: 0.10210137069225311\n",
      "Generator loss: 3.9842605590820312\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAABJCAYAAABxcwvcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABO9JREFUeJztmr1LHE0cxz/raV4UiUE9TgvFlxjORvTwOiGQMqKFjdhY\nBAQLu9ip+FIFxH8gtSGk8YVUEQsRrAyJlQHfCBFFxSSiJvE83afw+d24q96NcTTJ88wHlvXW3Znd\n337nN9+ZHcd1XSzJSfvdN/A3YIOkgQ2SBjZIGtggaWCDpIENkgY2SBrYIGmQfpmTHcf5z9lz13Wd\nVOdYJWlgg6SBDZIGNkga3FiQotEo0Wj0pqozilWSBs5lJt1+twUIBAIApKWdvNvDw8Mrl6ljAW48\nSB8+fACgqqpKyjxzTlZWFgD7+/tJy4pEIgC8e/ful+/H+iRD/HHNbXt7m9zcXACOj48B1byEO3fu\nAPDz588r12eVZIhLjd1M0NzcDMCrV68AlZOOjo4ACAaDFBQUAEpBfrVPTU0B8OjRIwBKS0sBWF5e\n9pznOE6ifFHlr2CVpIHRnOS67pn8EYvFAJU/xsfHAWhpaQFUty5vOhQKsb6+7imjs7MTgMHBQc/x\nvLw84CSPSf2pyMjIAJR9sDnJEMZ7t3g8DsDLly8B6O3tBWBpaSlV2Ym/xR+VlZUBsLGx4dkHg0FA\nKUfUIaoVNTuOkzhnZWUFgJKSEk+9VkmGMNq7BQKBxFv+8uULoPLIw4cPAcjOzgZgdnYWSO64BekJ\nQ6GQ5/jdu3cBpV5/qzj926+gy2CVpIHxnCT54O3bt4B6mxMTEwA8f/5cygKgv78fgJ6eHgCqq6t5\n//590jp2d3cBKC8vB2BoaAiA79+/A/D06VPgxJlLnroIm5MMcW1jt8bGRgBGR0cBpajFxUUAHjx4\nIGWeuXZ+fh6AcDictA7xWCMjI56yNzc3gRPVpno+qyRDGOndpHdJT1fFjY2NASp/TE5OAtDU1JSy\nvOHhYUCpT9T24sULANrb2z315uTkAMqZy/mmVvEZbW6RSIS5uTlAGbyZmRkAamtrAfVgKeoBLn7I\nHz9+AMoCiGU4ODjQrkOwzc0QRs3kkydPElOpMvUhXXBHRwcA+fn5ALx+/RqAjx8/At4JNFGQdOm3\nb98GSEzG3b9/31Ov1CWDZEnocvyqWCVpYDQnpaWlJVTw+PFjAD59+gTAwsJC0rKLiooA+Pr1ayLZ\nizEVNZ7uGEApRuqUSbi6ujoA+vr6Uj2SzUmmuDYzKW9Z8E+fmuieP3/+DEBlZSWg7IbfOiTDKskQ\n1zbAzczMBGBvb89fBpBcSYWFhQCsra15jov3kl5LBssyPdPa2prq9s5glWSIG/s42dDQAMCbN2+A\niz/x7OzscO/evXP/5/c/Mhz59u1b4lrgwuvPwyrJENfmk8TTpFr5ITks2cdDfx5LdY0oSZSVDKsk\nQxgdu7muS01NDaCWw6Ra3HCeGmTMJj2kKEj2MoaTjwq3bt0CYGtrC1A5yhRWSRoYV9Lq6upJwf/m\nJL+CUvmkWCzGs2fPPOfINbKvr68HVI/Z1tZm7BnOwypJA+NLb+RT9EWeRTyOf2HFadV0dXUByq37\nVVdRUQGcVdD09DSgZgFMcWNm0t90/GuPZF9cXJxosrK6RIYd3d3dAAwMDPzqbZx3X9YCGMF1Xe0N\ncE1v8Xjcjcfjid/hcNh1HMezXUe9suk8t1WSBn/c6tubxuYkQ9ggaWCDpMGlctL/FaskDWyQNLBB\n0sAGSQMbJA1skDSwQdLABkkDGyQNbJA0+AfklptjpicXMgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104707320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000\n",
      "Discriminator loss: 0.21843285858631134\n",
      "Generator loss: 3.7385568618774414\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAABJCAYAAABxcwvcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBykF8iClQB6kFMiDlAJ/A0rEL9ODHcn7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11afd65c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3000\n",
      "Discriminator loss: 0.4937973618507385\n",
      "Generator loss: 2.6894006729125977\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAABJCAYAAABxcwvcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABFtJREFUeJztmksotG0Yx3/D63ymNKWcdiyUsqAoGwtiIcTWTkkKe2VJ\nigUbW69YyMbOwsbCoRRCqSksCMn5EIZv4bvmfmc+3vcenpnp+77rtzPzPOOaa/7P//4/1/243t7e\nUH5PVKQL+DegTbJAm2SBNskCbZIF2iQLtEkWaJMs0CZZ8COYg10u138unr+9vbn+dIwqyQJtkgXa\nJAu0SRZokyzQJlmgTbIgqJwUCtLS0gC4vr4GwOPxUFhYCEBSUhIAnZ2dAHi9XgCGh4cBcLneI06o\np6uqJAtcwfwKTibumJgYAGZnZwGoq6vzvTc1NQVAR0cHAKenpwAkJycDcHNzA4Db7QaMwh4fH4Ou\nQxO3Q4TNk8Q/srKyADg5OQEgKur9dxJFu1wuUlNTAbi7uwPg+fkZgNfXV7/P8ng8AJSWlgJwfHwc\nktpVSRaEzZPOzs4Ao6TAlUn+9nq9Pu8Rj9nd3QWgpaUFgK2tLb/3l5aWAKipqQm6LvUkhwi5J4lC\nsrOzAbMSpaSkAGalEqW53W6f9whFRUUffnZcXBwAc3NzDlftjyrJgrB5kijq4uICMCqYnp4GoL29\n/dNzJZXLqre9vQ1AbW0tAIuLiwDExsb+49zo6GjAKDgQ9SSHCHvifnl5AUw+ury8BCAzM/PTcz6r\n8eDgADBpfWdnJ+h6VEkOETIllZeXA7CysgIYP5FfPycnBzCpWmhtbWVmZgYwCfvw8BCAvLw8wKyI\ngYk8NzfX+rsIqiSHCFlOWl5eBiAxMRGAkZERALq7uwE4OjoCYGBgAICSkhIAxsbG2NvbA8yKKAry\nFf3jvez09HQAEhISQvMl/iZsxj04OAhAb2+v3+sSCeSLV1VVsbm5+dvPkuApl6xcbl9BLzeHCHsE\nkFAnIS9w/PERDw8PgLmsGhoaAJifn/9uOaokpwjbDW6gYqurqwGzASC3Hh8hCpIg+pUx7XdQJVkQ\nNk+S25CKigrADMpk6L++vg7A0NCQLzaIF4lv3d/fA2Zw9/T09NVyfKgnOUTEtpQkRG5sbAAwOTkJ\nvPuPbEre3t4CRkmixsDNg++gSnKIsCtJVCBDN/GVX4dibW1tAPz8+dPvHDlG0rkM/hcWFr5cjyrJ\nIcL+wIT4i6xcHyE3v6OjowD09fUBUF9fDxglydaSeFjg2MUpVEkWRGx1k6xzfn7u93p+fr7vIYrK\nykrArIAFBQWAUUxGRgbwvVVOPckhIvYQV1dXFwD7+/sA9Pf3A1BcXMz4+DhgxrTiXzJkC5X3fIYq\nyYKwKUmyjmSbiYkJwGwMNDc3A3B1deVbvWSC0NjYCBhFyTaUKCvwfwRuk3+XsBl34E5qfHw8YG5a\nZQQbGxtLT08PAKurqwCsra35HSM1O9EUNW6HiFgEEOTSampqAqCsrMxn6jIy+fXZJadRJTlExJUU\naVRJDqFNskCbZEFQnvR/RZVkgTbJAm2SBdokC7RJFmiTLNAmWaBNskCbZIE2yYK/AAk7zo+lAwbR\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1152e4940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4000\n",
      "Discriminator loss: 1.354386806488037\n",
      "Generator loss: 2.4238088130950928\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAABJCAYAAABxcwvcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120bad4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000\n",
      "Discriminator loss: 1.3319649696350098\n",
      "Generator loss: 1.3066492080688477\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAABJCAYAAABxcwvcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120b6de80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6000\n",
      "Discriminator loss: 1.3624396324157715\n",
      "Generator loss: 1.3443700075149536\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAABJCAYAAABxcwvcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABQRJREFUeJzt2jmIFNsXx/FPjxvuO4rigoqikQxGgqDCRIqBuQsjIggD\nBkamIgZmCoIrBgaaGGgkKLgEYqQI6giiiLjivu/9AjldMz3z/7/bWj3je+9+k2Gqq25VnfrVub9z\nblWq1arM/6elvy/gn0AOUgI5SAnkICWQg5RADlICOUgJ5CAlkIOUwMBGdq5UKv86e16tVit/t09W\nUgI5SAnkICWQg5RADlICOUgJ/JFBam9v197e3t+XUaPSSGeyEZ9Uqfy0H6njz5w5s3bcsmXLwKdP\nn8CxY8e6jRV/W1paet3eCNknlUTTlNTlGPR8yoMHDwY/fvwAX758Ad+/f3fp0iWwZMkS8PbtW3D7\n9m2wcuVK8Pr1a/D+/XswcODPAuLbt2+184Ta4jz1ZCWVRNOUNHToUPD161cUT3fx4sVgwYIF4MCB\nA92Oq1arVqxYAfbu3QvGjRsHRo0ahZ9qg7Fjx6JQ0MuXL5Pvpcv5spLKoOk5adq0aeDBgwco8kso\nLbh48SJoa2ur5afHjx+jUMzw4cPBpEmTQEdHBzhy5Ai4d+8e+PjxY/L1ZSWVREP9pF/h/v37oLOz\nE5w9exbFDBWsXr0arF271tGjR8HkyZOhZixDjXPnzkWR52LsCRMmoDElpZCVlEDTc1IwYMAAFLPd\nvHnzULjqUFzd+VDMXlOmTAGzZ88G169fB0+ePEGR53JO6gdKyUmDBg1CoRIYMWIECjccio2nHO65\nNyWvX78eHD58GLx48QI8ffoUnDlzBpw+fRps2LABnDhxoozb6UFWUgJNz0m7du0Cu3fvRjFD1ddS\nXR36u3fvUCgkZrs7d+6AWbNmofBHBw8eBBcuXECRw1LuLeekkmiaksIlT5w4EWzZsgVs374dPHr0\nqNfjbt26Zc6cOejZL4qaLWbKUGPkxNGjR4NXr16lXmaSkppmJqPYfPPmDfj8+TP48OFDr/vHjVcq\nlZopjLLk2rVrYNGiRd3GuHHjBoogNhKcRsivWwKlv26hgufPn0OtFTt9+nQUyTY4dOgQ2LRpE2ht\nbXX+/Hnw8OFDMH/+fBR2Il7DKHSDUG0UyCnkxF0SpeekZ8+eoXja0YoNCxAJPf7funUrimJ1+fLl\ntbHWrFmDotV78+ZNMHLkSBQ5KwreUG/Q0dFhz549v31PWUkJNM0CxNO+evUqWLhwIVi3bh3YsWMH\nikI38k1nZ6epU6d2GytKmVBnmM2wBDNmzOi2XyPknFQSTfNJ0aaNtkaUI5s3b0Yx28WMFKVEvYoo\nWrutra0oFgaiSC67yVZPVlICTW/fBvv27QMnT55Ez+Kza5slCM/V1taGwi+F044lp3Did+/eRc9Z\n7nfJSkqgz9q3v0LMeMOGDeu2Pa45Wr/1vzdCnt1Kos9yUj31HzJ0/bAiaq9ogQT1n9z0FVlJCZSu\npFBEUJ/zli5dCs6dO9dt+7Zt28DOnTt7jBlqO378OH5t6eh3yEpKoHQl/a/ZMp5+VO6hglDe/v37\n8bMbUL9IMH78eBQdyUb6RWXQZxYgvgiJG718+TK4cuUK2LhxY23fWJGNptqQIUPQu+H8XbIFKIk/\nxkzG95GnTp2qFbBRJMc6WyisTLKSSqLfzGQ98ZXamDFjatsijwWxohuLB31FVlICf0xO6o1Vq1ah\naK80g5yTSqIhJf1XyUpKIAcpgRykBHKQEshBSiAHKYEcpARykBLIQUogBymBvwClvThuilZWPwAA\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120c39400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7000\n",
      "Discriminator loss: 1.3199636936187744\n",
      "Generator loss: 0.8287303447723389\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEkAAABJCAYAAABxcwvcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABVFJREFUeJzt2smPTN0fx/FXFZqOWSRiSEyRYGFasEBI7AyJjZAg2PoD\nOrG0EokVsSI2LIxBYl5IxLww9EaIFokQRAxtLHSrZ1E5dVTRntvc7v75Pee96VR31anTn/u53+nc\nQrlclvg1xZ7ewN9AEikDSaQMJJEykETKQBIpA0mkDCSRMpBEykDvzry5UCj835Xn5XK58G/vSU7K\nQBIpA0mkDCSRMtBtIhWLRcXi33lN/s5ddzOdKgH+hG/fvv3094VCJQOH4V+hUPC/NgjsNpGCGMOG\nDQOtra2gvb0djBo1Crx48cLXr187tWafPn3Aly9f8tvwd6TbLQNd7qRwlWfPng2mTp0KmpubwfTp\n08H8+fPBmTNnHD9+HAwfPhycPHkSLFu2DDx8+BAMGTIEvH37Fj/eunmRnJSBQmdU76h3a2hoQG1M\nGD16NDh06BC4d+9eWAPRUZMmTUJ0VnNzsytXroANGzaAefPmgRs3btSsefToUcT49uHDB/zaSb16\n9UKMhal3y4lcnPTd36sFY79+/cCxY8fAx48fEd129+5dxCt64sQJVOLL6dOnER06aNAg0L9/f/D5\n82ewYMECcOvWrZq1OkNyUk7k6qSGhoaaohC2bt0KLl68CPbv34+YoZYvXw5evXqFitNCFtu4cSOY\nOXMmeP78OWJh+uDBA7B+/Xrw5MmTmr9nITkpJ3J1UrFYrF7FoUOHImaTkO0mTpwIzp07h5iJQixr\na2uzefNmcOnSJbB9+3Zw9uxZsHjxYjBhwoSa729ra6tZMwvJSTmRq5OIjujbty+ic96/fw/evXsH\nSqUSfuy32tvbjRs3DrEOmjJlCnj06BFi7Fm1ahV49uwZfq/STk7KiS7r3YJTQqU9ePBgMHDgQMS6\nKVz9kSNHolIDzZkzB7F327FjB2KfN2LECMTe7enTp131byA5KRO5Oylkt1AnhYwTnPPy5Uuwc+dO\nsGXLFsRerqmpybZt28CaNWsQXbZy5Upw4cIFdN38qJ7cA/cvPovYWoRbJoi2du1aVJrVIFxobe7f\nvw8mT54MNm3ahNj4rl69+ne3lQJ3XnSZk3r3rtzJ4XYLpUEoLpuamhBHJLt27UKlUHz9+jWYNWsW\nOHz4MGKQf/z4MWJ5Eb7jd0hOyoncnRRiT6DeUYExY8YgNq9z584Fd+7cceTIEXDz5k3E8Wxodfbt\n24cY9EO58bO9/Nv/l5yUE7mXAOHKhdgTjofqHRay2vnz56E6si2VStXCMzSwYY0wVAsurB+y1R8E\nNDY2VkuPPyE5KQNdlt1CNss6AAvxprW1tTqQCzVVaD/evHkDxo4dizj4/25/Na/L5fJPDynq3pNi\nUh50WYOb1UHBccElLS0t1TakfpwShm+hSQ5O+pmDAlmPzH+5xz9e4T9Atz0wUU9wUP3P8ePHV4+M\nGhsbwbVr18DBgwcRh2zBQSGThlosfK5UKv1wMPE7g7nkpAx02xSg/nj5uzXB3r17wZIlS6rZLOxt\nwIABiDGqPt79iUtSdsuJHn/SLRxhh9lRY2NjNZvdvn0bcfQbHuMJWS28/vTpE37McoVCoVMHlR2R\nnJSBbnNSR/Ei9FYzZsyovg6uamlpAQsXLgQHDhyo+WxwUP13BEfl4SJ6sAQILFq0CKxYsQJcv37d\nqVOnEJ9w62yLk5506wF6zEnhlrh69SpYt24dKg3u0qVLwe7du8Hly5d7YIeR5KQMdFsx2RFh7LFn\nzx5URrPhyGjatGnoeDybB6mYzIked1JPk5yUE51y0n+V5KQMJJEykETKQBIpA0mkDCSRMpBEykAS\nKQNJpAwkkTLwDxwlW5SVAh2vAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120b6e0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    i = 0\n",
    "    for epoch in range(100000):\n",
    "        X_mb, _ = mnist.train.next_batch(128)\n",
    "        _, cur_loss_d = sess.run([d_step, d_loss], feed_dict = {X: X_mb, Z: rand_noise_vector(128, 100)})\n",
    "        _, cur_loss_g = sess.run([g_step, g_loss], feed_dict = {Z: rand_noise_vector(128, 100)})\n",
    "        _, cur_loss_g = sess.run([g_step, g_loss], feed_dict = {Z: rand_noise_vector(128, 100)})\n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"Epoch: {}\".format(epoch))\n",
    "            print(\"Discriminator loss: {}\".format(cur_loss_d))\n",
    "            print(\"Generator loss: {}\".format(cur_loss_g))\n",
    "            samples = sess.run(g_sample, feed_dict={Z: rand_noise_vector(1, 100)})\n",
    "            plot(samples, epoch)\n",
    "    samples = sess.run(g_sample, feed_dict={Z: rand_noise_vector(16, 100)})\n",
    "    plot(samples) # 16 of em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Exercises\n",
    "- Try changing up the discriminator model. In particular, what happens if you add more hidden or convolutional layers to the discriminator network?\n",
    "- Read up about deconvolutions/transposed convolutions and try to implement a convolutional architecture for the generator. If you do this, you will get something close to the DCGAN architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Closing Thoughts\n",
    "In this post, we showed how two networks were able to play a minimax game in order to capture the data distribution of the MNIST digits and generate similar looking samples. With applications in video frame prediction, text-image mappings, and more, GANs are definitely the hottest topic in deep learning. Hopefully, with this tutorial, you’ve gained a better understanding of how these networks work in practice and how you can build your own with Tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
